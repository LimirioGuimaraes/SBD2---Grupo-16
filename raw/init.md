# PC1 - Até a criação/documentação do lakehouse 
- Criação do git 
- Escolha dos dados 		
- Documentação da camada bronze 
- Dicionário de dados - Job python para explorar os dados 
- analytcs notebook python com análise desses dados(pandas.read.csv, qtd nulos etc) 
- Na camada bronze ainda, sem tratar o dado 
- MER, DER, DLD, DDL(script de criação da tabela), Dicionário de dados da camada silver 
- Job ETL (rall -> Silver), vai puxar da bronze e jogar na silver 
- Ver o Lakehouse populado 
- Lakehouse - Deve ter indice, com indices que façam sentido 
- Postgress(com page admin não dbeaver) 
- Critérios, código bem feito e bem documentado 
- Docker - Docker compose up(já rodar o jupyter para popular a tabela e está tudo rodando) 